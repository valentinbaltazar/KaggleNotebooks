{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4622a6f5",
   "metadata": {
    "papermill": {
     "duration": 0.009073,
     "end_time": "2024-09-16T17:42:46.646937",
     "exception": false,
     "start_time": "2024-09-16T17:42:46.637864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gemma 7B-it Template\n",
    "\n",
    "Hi KaggleX team, this is a template for those that are having trouble with Gemma 7b which is hard to run because of the GPU requirements.\n",
    "\n",
    "In this notebook we run the quantized (int8) model of Gemma 7B Instruction-tunned ('7b-it-quant'), this basically means\n",
    "we can run the 7B model on just 1 GPU (Here on Kaggle select either a P100 or a T4 GPU).\n",
    "\n",
    "**Things to keep in mind**:\n",
    "* This notebook defaults to the Gemma '7b-it-quant' but I have added code so if you want to use the 2b-it model you can also.\n",
    "* Run this notebook on Kaggle, if you want to use it on google collab you will need to modify some parts but it can be done.\n",
    "* This is a PyTorch implementation, not Keras\n",
    "* I have added comments where you should, and should not customize the code for your own projects.\n",
    "\n",
    "\n",
    "*Best of luck! - Valentin, KaggleX Team D*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc7809",
   "metadata": {
    "papermill": {
     "duration": 0.007948,
     "end_time": "2024-09-16T17:42:46.663022",
     "exception": false,
     "start_time": "2024-09-16T17:42:46.655074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Before you start:** Make sure to pick an accelerator (GPU) --> Either a P100 or the T4x2 on Kaggle work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb668f",
   "metadata": {
    "papermill": {
     "duration": 0.007803,
     "end_time": "2024-09-16T17:42:46.678724",
     "exception": false,
     "start_time": "2024-09-16T17:42:46.670921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-- Run this cell which will set up needed directories and download some requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae07661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:42:46.697257Z",
     "iopub.status.busy": "2024-09-16T17:42:46.696264Z",
     "iopub.status.idle": "2024-09-16T17:43:06.841825Z",
     "shell.execute_reply": "2024-09-16T17:43:06.840666Z"
    },
    "papermill": {
     "duration": 20.157496,
     "end_time": "2024-09-16T17:43:06.844338",
     "exception": false,
     "start_time": "2024-09-16T17:42:46.686842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\r\n",
      "remote: Enumerating objects: 239, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (123/123), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (68/68), done.\u001b[K\r\n",
      "remote: Total 239 (delta 86), reused 58 (delta 55), pack-reused 116 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (239/239), 2.18 MiB | 11.85 MiB/s, done.\r\n",
      "Resolving deltas: 100% (135/135), done.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U immutabledict sentencepiece \n",
    "!git clone https://github.com/google/gemma_pytorch.git\n",
    "!mkdir /kaggle/working/gemma/\n",
    "!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3dcab",
   "metadata": {
    "papermill": {
     "duration": 0.007687,
     "end_time": "2024-09-16T17:43:06.860474",
     "exception": false,
     "start_time": "2024-09-16T17:43:06.852787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-- Run this to import the needed python packages and set the path variables needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ccfb12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:06.878615Z",
     "iopub.status.busy": "2024-09-16T17:43:06.878154Z",
     "iopub.status.idle": "2024-09-16T17:43:11.142679Z",
     "shell.execute_reply": "2024-09-16T17:43:11.141703Z"
    },
    "papermill": {
     "duration": 4.276698,
     "end_time": "2024-09-16T17:43:11.145189",
     "exception": false,
     "start_time": "2024-09-16T17:43:06.868491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports and path setup\n",
    "import sys \n",
    "sys.path.append(\"/kaggle/working/gemma_pytorch/\") \n",
    "from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "import contextlib\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Do not modify, used for Pytorch\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d2a2a",
   "metadata": {
    "papermill": {
     "duration": 0.008372,
     "end_time": "2024-09-16T17:43:11.162265",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.153893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1. Create Fromatter.\n",
    "We first need to make the formatter which is used for the 'instruction' tunned models, these **\\<TOKENS>** are *required*, so dont modify unless you no longer want to use the intruction tunned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93477321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:11.182159Z",
     "iopub.status.busy": "2024-09-16T17:43:11.180945Z",
     "iopub.status.idle": "2024-09-16T17:43:11.192403Z",
     "shell.execute_reply": "2024-09-16T17:43:11.191343Z"
    },
    "papermill": {
     "duration": 0.023882,
     "end_time": "2024-09-16T17:43:11.194705",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.170823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class for formatting the input data\n",
    "class GemmaFormatter:\n",
    "    \"\"\"This class is for the special case of the instruction-tunned models, which require these tokens.\"\"\"\n",
    "    \n",
    "    # DO NOT modify these tokens, they are needed for the 7b-it and 2b-it models (instruction tunned)\n",
    "    _start_of_turn_user = \"<start_of_turn>user\\n\"\n",
    "    _start_of_turn_model = \"<start_of_turn>model\\n\"\n",
    "    _end_of_turn = \"<end_of_turn>\\n\"\n",
    "    def __init__(self, user_instruct = None, model_response = None):\n",
    "        self._user_instruct = user_instruct\n",
    "        self._model_response = model_response\n",
    "        self.dataset = []\n",
    "        \n",
    "    def user_input(self, prompt):\n",
    "        # This will format a single user prompt\n",
    "        user_prompt =  self._start_of_turn_user + prompt + \"\\n\" + self._end_of_turn + self._start_of_turn_model\n",
    "        return user_prompt\n",
    "        \n",
    "    def apply_format(self):\n",
    "        # Will retunr the full dataset with tokens applied to both user input and model response\n",
    "        for user_ins, model_res in zip(self._user_instruct, self._model_response):\n",
    "            user_prompt =  self._start_of_turn_user + user_ins + \"\\n\" + self._end_of_turn\n",
    "            model_prompt = self._start_of_turn_model + model_res + \"\\n\" + self._end_of_turn\n",
    "            print(user_prompt)\n",
    "            print(model_prompt)\n",
    "\n",
    "            self.dataset.append([user_prompt, model_prompt])\n",
    "            \n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bcc2f",
   "metadata": {
    "papermill": {
     "duration": 0.009036,
     "end_time": "2024-09-16T17:43:11.212621",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.203585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will now test the formatter, the way it works is you input your data in the form of two arrays, and it will format and return the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5af923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:11.233339Z",
     "iopub.status.busy": "2024-09-16T17:43:11.232293Z",
     "iopub.status.idle": "2024-09-16T17:43:11.238467Z",
     "shell.execute_reply": "2024-09-16T17:43:11.237527Z"
    },
    "papermill": {
     "duration": 0.019344,
     "end_time": "2024-09-16T17:43:11.241062",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.221718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: this is an example, you want to modify these arrays with your dataset\n",
    "# These are list for the desired user inputs and model response\n",
    "# This is how I want my chess bot to responed, based on the user_input_array\n",
    "\n",
    "user_input_array = [\"Narrate this chess pgn game: 1. e4 e5 2. Nf3 Nc6 3. O-O d3\", \"Narrate this chess pgn game: 1. e4 c5 2. Nf3 g6 3. Ne5 Bg7\"]\n",
    "model_output_array = [\"\"\"White starts of with pawn to e4, balck responds with pawn to e5. White then plays\n",
    "Knight to f3, black plays Knight c6. White castles king side, and black plays d3.\"\"\", \n",
    "           \"\"\"White starts of with pawn to e4, balck responds with pawn c5. White then plays\n",
    "Knight to f3, black plays pawn to g6. White continues with Knight e5, black counters with Bishop g7.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca7b036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:11.263401Z",
     "iopub.status.busy": "2024-09-16T17:43:11.262966Z",
     "iopub.status.idle": "2024-09-16T17:43:11.269101Z",
     "shell.execute_reply": "2024-09-16T17:43:11.268038Z"
    },
    "papermill": {
     "duration": 0.018605,
     "end_time": "2024-09-16T17:43:11.271684",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.253079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "Narrate this chess pgn game: 1. e4 e5 2. Nf3 Nc6 3. O-O d3\n",
      "<end_of_turn>\n",
      "\n",
      "<start_of_turn>model\n",
      "White starts of with pawn to e4, balck responds with pawn to e5. White then plays\n",
      "Knight to f3, black plays Knight c6. White castles king side, and black plays d3.\n",
      "<end_of_turn>\n",
      "\n",
      "<start_of_turn>user\n",
      "Narrate this chess pgn game: 1. e4 c5 2. Nf3 g6 3. Ne5 Bg7\n",
      "<end_of_turn>\n",
      "\n",
      "<start_of_turn>model\n",
      "White starts of with pawn to e4, balck responds with pawn c5. White then plays\n",
      "Knight to f3, black plays pawn to g6. White continues with Knight e5, black counters with Bishop g7.\n",
      "<end_of_turn>\n",
      "\n",
      "[['<start_of_turn>user\\nNarrate this chess pgn game: 1. e4 e5 2. Nf3 Nc6 3. O-O d3\\n<end_of_turn>\\n', '<start_of_turn>model\\nWhite starts of with pawn to e4, balck responds with pawn to e5. White then plays\\nKnight to f3, black plays Knight c6. White castles king side, and black plays d3.\\n<end_of_turn>\\n'], ['<start_of_turn>user\\nNarrate this chess pgn game: 1. e4 c5 2. Nf3 g6 3. Ne5 Bg7\\n<end_of_turn>\\n', '<start_of_turn>model\\nWhite starts of with pawn to e4, balck responds with pawn c5. White then plays\\nKnight to f3, black plays pawn to g6. White continues with Knight e5, black counters with Bishop g7.\\n<end_of_turn>\\n']]\n"
     ]
    }
   ],
   "source": [
    "# Test the formatter class so you can see how it works\n",
    "test_formatter = GemmaFormatter(user_input_array, model_output_array)\n",
    "\n",
    "data = test_formatter.apply_format()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502df28d",
   "metadata": {
    "papermill": {
     "duration": 0.007917,
     "end_time": "2024-09-16T17:43:11.287500",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.279583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2. Create Agent\n",
    "\n",
    "Now we make our 'Agent' class, this is the llm model we want to feed our test prompt and see the results, and/or fine tune if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2948ce52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:11.305242Z",
     "iopub.status.busy": "2024-09-16T17:43:11.304858Z",
     "iopub.status.idle": "2024-09-16T17:43:11.316274Z",
     "shell.execute_reply": "2024-09-16T17:43:11.315151Z"
    },
    "papermill": {
     "duration": 0.023034,
     "end_time": "2024-09-16T17:43:11.318604",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.295570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaAgent:\n",
    "    \"\"\"This is the Agent which we will load into the GPU device and then call the LLM model and get a response\"\"\"\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', user_instruct=None,\n",
    "                 model_response=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(user_instruct = user_instruct, model_response = model_response)\n",
    "        \n",
    "        # Initialize model variant\n",
    "        print(\"Initializing model...\")\n",
    "        _weights_dir  = '/kaggle/input/gemma/pytorch/2b-it/2' if \"2b\" in variant else '/kaggle/input/gemma/pytorch/7b-it-quant/2'\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(_weights_dir, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            print(\"Default dtype set to:\", model_config.get_dtype())\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(_weights_dir , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "    \n",
    "    # This function will call the 'generate' function of your model\n",
    "    def call_llm(self, prompt, max_tokens=100, **sampler_kwargs):\n",
    "        print(\"Formatting your input prompt with tokens...\")\n",
    "        prompt = self.formatter.user_input(prompt)\n",
    "        print(prompt)\n",
    "        \n",
    "        if sampler_kwargs is None:\n",
    "            # IMPORTANT: Modify these paramaters for your LLM or add/remove as needed, these are defaults\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.1,\n",
    "                'top_p': 0.5,\n",
    "                'top_k': 10\n",
    "            }\n",
    "\n",
    "        # This is the output from the model\n",
    "        response = self.model.generate(\n",
    "        prompt,\n",
    "        device=self._device,\n",
    "        output_len=max_tokens,\n",
    "        **sampler_kwargs)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52170a",
   "metadata": {
    "papermill": {
     "duration": 0.009499,
     "end_time": "2024-09-16T17:43:11.337136",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.327637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ok, now we have the formatter and the model, so lets load it into memory and test that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f8c482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:11.357269Z",
     "iopub.status.busy": "2024-09-16T17:43:11.356761Z",
     "iopub.status.idle": "2024-09-16T17:43:55.347343Z",
     "shell.execute_reply": "2024-09-16T17:43:55.346214Z"
    },
    "papermill": {
     "duration": 44.003476,
     "end_time": "2024-09-16T17:43:55.349989",
     "exception": false,
     "start_time": "2024-09-16T17:43:11.346513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Default dtype set to: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Load the GemmaAgent class as 'my_chat_bot', with variant='7b-it-quant' or '2b-it'\n",
    "my_chat_bot = GemmaAgent(variant='7b-it-quant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c01d2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:55.369835Z",
     "iopub.status.busy": "2024-09-16T17:43:55.369426Z",
     "iopub.status.idle": "2024-09-16T17:43:55.374594Z",
     "shell.execute_reply": "2024-09-16T17:43:55.373596Z"
    },
    "papermill": {
     "duration": 0.018349,
     "end_time": "2024-09-16T17:43:55.377375",
     "exception": false,
     "start_time": "2024-09-16T17:43:55.359026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narrate this chess pgn game: 1. e4 e5 2. Nf3 Nc6 3. O-O d3\n"
     ]
    }
   ],
   "source": [
    "# Lets use a sample test prompt\n",
    "test_prompt = user_input_array[0]\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5243be54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:43:55.397822Z",
     "iopub.status.busy": "2024-09-16T17:43:55.397389Z",
     "iopub.status.idle": "2024-09-16T17:44:35.861087Z",
     "shell.execute_reply": "2024-09-16T17:44:35.859985Z"
    },
    "papermill": {
     "duration": 40.485454,
     "end_time": "2024-09-16T17:44:35.872383",
     "exception": false,
     "start_time": "2024-09-16T17:43:55.386929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting your input prompt with tokens...\n",
      "<start_of_turn>user\n",
      "Narrate this chess pgn game: 1. e4 e5 2. Nf3 Nc6 3. O-O d3\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Sure, here's the narrative of the game:\n",
      "\n",
      "The white pieces start with the Queen's pawn and the knight moves forward to challenge the pawn's control. The queenside castling moves the king and the queen to the opposite sides of the king, creating a symmetrical position. The black pieces respond with the development of the dark-bishop and the dramatic move of moving the pawn to d3, aiming to create a kingside counter-gambit.\n"
     ]
    }
   ],
   "source": [
    "# This will format the prompt above and call the llm and print response\n",
    "response = my_chat_bot.call_llm(test_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888f8ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:44:35.893306Z",
     "iopub.status.busy": "2024-09-16T17:44:35.892141Z",
     "iopub.status.idle": "2024-09-16T17:46:01.438518Z",
     "shell.execute_reply": "2024-09-16T17:46:01.437465Z"
    },
    "papermill": {
     "duration": 85.568414,
     "end_time": "2024-09-16T17:46:01.450306",
     "exception": false,
     "start_time": "2024-09-16T17:44:35.881892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting your input prompt with tokens...\n",
      "<start_of_turn>user\n",
      "Narrate this chess pgn game: 1. e4 e5 2. Nf3 Nc6 3. O-O d3\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Sure, here's the narrative of the game:\n",
      "\n",
      "The white pieces start the game by moving the e-pawn forward to e5 and the knight moves to f3 to challenge the pawn. The black pieces respond with the move d3 to counter the pawn advance and to bring the king out of the back row.\n",
      "\n",
      "The moves that follow are the standard opening moves of the Queen's Pawn Game, but the black side has chosen the Najdorf Variation of the Queen's Pawn Game, which is a popular variation that leads to a variety of positions.\n"
     ]
    }
   ],
   "source": [
    "# Now lets try the same prompt with different paramaters\n",
    "sampler_kwargs = {\n",
    "    'temperature': 0.3,\n",
    "    'top_p': 0.3,\n",
    "    'top_k': 5}\n",
    "\n",
    "response = my_chat_bot.call_llm(test_prompt, \n",
    "                                max_tokens=200,\n",
    "                                **sampler_kwargs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c9afa",
   "metadata": {
    "papermill": {
     "duration": 0.008982,
     "end_time": "2024-09-16T17:46:01.468276",
     "exception": false,
     "start_time": "2024-09-16T17:46:01.459294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### OK! Cool, it does as instructed and gives a good story! (although it does not understand specefic chess moves, and has some wrong interpretations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93cef3d",
   "metadata": {
    "papermill": {
     "duration": 0.009491,
     "end_time": "2024-09-16T17:46:01.486832",
     "exception": false,
     "start_time": "2024-09-16T17:46:01.477341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The End - have fun testing these models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198811d0",
   "metadata": {
    "papermill": {
     "duration": 0.008923,
     "end_time": "2024-09-16T17:46:01.504831",
     "exception": false,
     "start_time": "2024-09-16T17:46:01.495908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## --- Step 3. LoRA (NOTE: This does not work....if someone can fix it ty :)\n",
    "\n",
    "Now lets try this again but after we fine tune with LoRA on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e2dc4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:46:01.525380Z",
     "iopub.status.busy": "2024-09-16T17:46:01.524905Z",
     "iopub.status.idle": "2024-09-16T17:46:01.530505Z",
     "shell.execute_reply": "2024-09-16T17:46:01.529520Z"
    },
    "papermill": {
     "duration": 0.018686,
     "end_time": "2024-09-16T17:46:01.532648",
     "exception": false,
     "start_time": "2024-09-16T17:46:01.513962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reminder this is an example dataset from my own project\n",
    "# Replace with your dataset\n",
    "\n",
    "user_input_array = [\"Narrate this chess pgn game: 1. e4 e5 2. Nf3 Nc6 3. O-O d3\", \"Narrate this chess pgn game: 1. e4 c5 2. Nf3 g6 3. Ne5 Bg7\"]\n",
    "model_output_array = [\"\"\"White starts of with pawn to e4, balck responds with pawn to e5. White then plays\n",
    "Knight to f3, black plays Knight c6. White castles king side, and black plays d3.\"\"\", \n",
    "           \"\"\"White starts of with pawn to e4, balck responds with pawn c5. White then plays\n",
    "Knight to f3, black plays pawn to g6. White continues with Knight e5, black counters with Bishop g7.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446d30b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:46:01.552719Z",
     "iopub.status.busy": "2024-09-16T17:46:01.552298Z",
     "iopub.status.idle": "2024-09-16T17:46:16.604109Z",
     "shell.execute_reply": "2024-09-16T17:46:16.602687Z"
    },
    "papermill": {
     "duration": 15.064987,
     "end_time": "2024-09-16T17:46:16.606716",
     "exception": false,
     "start_time": "2024-09-16T17:46:01.541729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to install a new package for LoRA use\n",
    "!pip install -q -U peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d7f15c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:46:16.628051Z",
     "iopub.status.busy": "2024-09-16T17:46:16.627031Z",
     "iopub.status.idle": "2024-09-16T17:46:19.307743Z",
     "shell.execute_reply": "2024-09-16T17:46:19.306508Z"
    },
    "papermill": {
     "duration": 2.694396,
     "end_time": "2024-09-16T17:46:19.310388",
     "exception": false,
     "start_time": "2024-09-16T17:46:16.615992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages needed for LoRA\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38437a",
   "metadata": {
    "papermill": {
     "duration": 0.009065,
     "end_time": "2024-09-16T17:46:19.328851",
     "exception": false,
     "start_time": "2024-09-16T17:46:19.319786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets take a look at our model architecture, so we find what layers we apply LoRA to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c86c573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:46:19.350448Z",
     "iopub.status.busy": "2024-09-16T17:46:19.349158Z",
     "iopub.status.idle": "2024-09-16T17:46:19.359516Z",
     "shell.execute_reply": "2024-09-16T17:46:19.358504Z"
    },
    "papermill": {
     "duration": 0.023473,
     "end_time": "2024-09-16T17:46:19.361732",
     "exception": false,
     "start_time": "2024-09-16T17:46:19.338259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of GemmaForCausalLM(\n",
       "  (embedder): Embedding()\n",
       "  (model): GemmaModel(\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (qkv_proj): Linear()\n",
       "          (o_proj): Linear()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear()\n",
       "          (up_proj): Linear()\n",
       "          (down_proj): Linear()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (sampler): Sampler()\n",
       ")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chat_bot.model.named_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e09add",
   "metadata": {
    "papermill": {
     "duration": 0.009404,
     "end_time": "2024-09-16T17:46:19.380441",
     "exception": false,
     "start_time": "2024-09-16T17:46:19.371037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ok looks like these are the attention layers: \n",
    "```(self_attn): GemmaAttention(\n",
    "          (qkv_proj): Linear()\n",
    "          (o_proj): Linear()\n",
    "        )```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bb6ac",
   "metadata": {
    "papermill": {
     "duration": 0.008654,
     "end_time": "2024-09-16T17:46:19.399138",
     "exception": false,
     "start_time": "2024-09-16T17:46:19.390484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Do not run, will give errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da05b3ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T17:46:19.419121Z",
     "iopub.status.busy": "2024-09-16T17:46:19.418198Z",
     "iopub.status.idle": "2024-09-16T17:46:19.423093Z",
     "shell.execute_reply": "2024-09-16T17:46:19.422274Z"
    },
    "papermill": {
     "duration": 0.017091,
     "end_time": "2024-09-16T17:46:19.425033",
     "exception": false,
     "start_time": "2024-09-16T17:46:19.407942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# # LoRA configuration\n",
    "# peft_config = LoraConfig(\n",
    "# #     task_type=TaskType.CAUSAL_LM,\n",
    "#     inference_mode=False,\n",
    "#     target_modules=[\"qkv_proj\", \"o_proj\"],  # Specify the target modules where you want to apply LoRA\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.1\n",
    "# )\n",
    "\n",
    "# # Apply LoRA to our model from above\n",
    "# lora_chat_bot = get_peft_model(my_chat_bot.model, peft_config)\n",
    "\n",
    "# # Move the model to GPU if available, (Assumes a T4x2)\n",
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# lora_chat_bot.to(device)\n",
    "\n",
    "# # Now the model is ready with LoRA applied\n",
    "# print(lora_chat_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077a46c",
   "metadata": {
    "papermill": {
     "duration": 0.009041,
     "end_time": "2024-09-16T17:46:19.443458",
     "exception": false,
     "start_time": "2024-09-16T17:46:19.434417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 3301,
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 3301,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 217.9115,
   "end_time": "2024-09-16T17:46:21.282255",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-16T17:42:43.370755",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
